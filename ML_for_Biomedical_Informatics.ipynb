{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a1ren-code/PyWGCNA/blob/main/ML_for_Biomedical_Informatics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We'll be using a powerful library called \"scikit-learn\" (sklearn), which has already implemented all the machine learning models we've talked about. I'd say >80% of all ML workflows use sklearn, so this is the industry standard!"
      ],
      "metadata": {
        "id": "nXJ7nNbhPxH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZihokGDdFSmw"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import datasets\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "# Import ML packages\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning\n",
        "\n",
        "\n",
        "\n",
        "Generally speaking, there are two types of supervised learning tasks you will encounter: classification and regression. This notebook will give you a taste of each.\n",
        "\n",
        "- Classification: Want to predict what \"class\" a data point belongs to (ex: malignant or benign tumor)\n",
        "- Regression: Want to predict a quantitative value (ex: diabetes risk score)"
      ],
      "metadata": {
        "id": "hq_1TroPQpPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "\n",
        "For this example, we'll use the breast cancer dataset from sklearn. Each row is a tumor sample, and each column is a feature related to the tumor (ex: size, volume). The target variable is whether the tumor is benign or malignant (0 or 1).\n",
        "\n",
        "We want to find the model, and combination of features, that leads to the best prediction of whether someone has a malignant (AKA cancerous) tumor!"
      ],
      "metadata": {
        "id": "LfiKCbRhnkcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_cancer = load_breast_cancer()\n",
        "\n",
        "# Preview data\n",
        "df_cancer = pd.DataFrame(data_cancer.data, columns=data_cancer.feature_names)\n",
        "print(df_cancer.shape)\n",
        "print(list(data_cancer.target_names))\n",
        "df_cancer.head()"
      ],
      "metadata": {
        "id": "QgPNSu4iDHZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_cancer.data, data_cancer.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "VogUoHl5nKx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "This is the simplest option for binary classification."
      ],
      "metadata": {
        "id": "LOuIbJWcWEfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr, target_names=data_cancer.target_names))\n",
        "ConfusionMatrixDisplay.from_estimator(lr_model, X_test, y_test, display_labels=data_cancer.target_names, cmap='Blues')\n",
        "plt.title(\"Logistic Regression: Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CUsgpsWaWKpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron"
      ],
      "metadata": {
        "id": "MKJ7NFkOWhT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perc_model = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
        "perc_model.fit(X_train, y_train)\n",
        "y_pred_perc = perc_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_perc, target_names=data_cancer.target_names))\n",
        "ConfusionMatrixDisplay.from_estimator(perc_model, X_test, y_test, display_labels=data_cancer.target_names, cmap='Greens')\n",
        "plt.title(\"Perceptron: Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vIbkCspEn2TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "mTroZIiJWSz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plot_tree(dt_model, feature_names=data_cancer.feature_names, class_names=data_cancer.target_names, filled=True)\n",
        "plt.title(\"Decision Tree Logic Flow\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jFhpqkoEq2uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "TVKc_4iOWURV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Show Feature Importance, what matters the most?\n",
        "importances = pd.Series(rf_model.feature_importances_, index=data_cancer.feature_names).sort_values(ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=importances.head(10), y=importances.head(10).index)\n",
        "plt.title(\"Top 10 Clinical Features (Random Forest)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t5ubYz6yq_ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Best Model?\n"
      ],
      "metadata": {
        "id": "Phd_IxWGrEII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Plot each model's ROC curve\n",
        "RocCurveDisplay.from_estimator(lr_model, X_test, y_test, ax=ax, name=\"Logistic Regression\")\n",
        "RocCurveDisplay.from_estimator(dt_model, X_test, y_test, ax=ax, name=\"Decision Tree\")\n",
        "RocCurveDisplay.from_estimator(rf_model, X_test, y_test, ax=ax, name=\"Random Forest\")\n",
        "RocCurveDisplay.from_estimator(perc_model, X_test, y_test, ax=ax, name=\"Perceptron\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "plt.title(\"Comparison of Diagnostic Models (ROC Curve)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eaM7HyWTrDfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have run predictions for all models:\n",
        "# y_pred_lr, y_pred_perc, y_pred_dt, y_pred_rf\n",
        "\n",
        "class_results = [\n",
        "    {\n",
        "        \"Model\": \"Logistic Regression\",\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_lr),\n",
        "        \"Recall (Sensitivity)\": recall_score(y_test, y_pred_lr),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred_lr)\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Perceptron\",\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_perc),\n",
        "        \"Recall\": recall_score(y_test, y_pred_perc),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred_perc)\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Decision Tree\",\n",
        "        \"Model\": \"Decision Tree\",\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_dt),\n",
        "        \"Recall\": recall_score(y_test, y_pred_dt),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred_dt)\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Random Forest\",\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_rf),\n",
        "        \"Recall\": recall_score(y_test, y_pred_rf),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred_rf)\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create and Sort by Recall (In medicine, we want to catch every case!)\n",
        "summary_df = pd.DataFrame(class_results).sort_values(by=\"Recall (Sensitivity)\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"--- Clinical Classification Leaderboard ---\")\n",
        "print(summary_df)\n",
        "\n",
        "# Identify the winner based on the highest Recall\n",
        "best_model_name = summary_df.iloc[0]['Model']\n",
        "print(f\"The Best Model is: {best_model_name}\")"
      ],
      "metadata": {
        "id": "23FEDFjUz4Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "\n",
        "In this example, we'll use sklearn's diabetes dataset. Each row is a patient with diabetes, and each column is a feature relevant to diabetes.\n",
        "\n",
        "The goal is to predict a quantitative measure of disease progression one year after a set of baseline measurements were taken!"
      ],
      "metadata": {
        "id": "SCSL2T9_nhqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_diabetes = load_diabetes()\n",
        "\n",
        "# Preview data\n",
        "df_diabetes = pd.DataFrame(data_diabetes.data, columns=data_diabetes.feature_names)\n",
        "print(df_diabetes.shape)\n",
        "print(list(data_diabetes.target))\n",
        "df_diabetes.head()\n"
      ],
      "metadata": {
        "id": "iMq6D1INDvnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "X, y = data_diabetes.data, data_diabetes.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NqJGSsgJv7tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "Pretty much always our first choice for any regression task."
      ],
      "metadata": {
        "id": "1Qkis0l2s45W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "y_pred_lin = lin_reg.predict(X_test)\n",
        "\n",
        "print(f\"Linear Regression R^2 Score: {r2_score(y_test, y_pred_lin):.2f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_lin):.2f} points\")\n",
        "\n",
        "# Plotting Actual vs Predicted\n",
        "plt.scatter(y_test, y_pred_lin, alpha=0.5)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
        "plt.xlabel(\"Actual Progression\")\n",
        "plt.ylabel(\"Predicted Progression\")\n",
        "plt.title(\"Linear Regression: Actual vs Predicted\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2rvHsgJstniu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized Linear Regression"
      ],
      "metadata": {
        "id": "Je4VfAYiuCJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred_lasso)\n",
        "r2 = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(f\"Lasso Regression Results:\")\n",
        "print(f\"R^2 Score: {r2:.3f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")"
      ],
      "metadata": {
        "id": "OOFE-W7qxFqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show which features Lasso kept (non-zero coefficients)\n",
        "feature_importance = pd.Series(lasso_model.coef_, index=data_diabetes.feature_names)\n",
        "print(\"Lasso Coefficients (importance of each feature):\")\n",
        "print(feature_importance.sort_values(ascending=False))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "feature_importance.plot(kind='barh')\n",
        "plt.title(\"Lasso Regression: Feature Impact\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DXVrncP1uBmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree\n",
        "\n"
      ],
      "metadata": {
        "id": "4RnqWqUVulsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "\n",
        "print(f\"Decision Tree R^2 Score: {r2_score(y_test, y_pred_dt):.2f}\")\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plot_tree(dt_reg, feature_names=data_diabetes.feature_names, filled=True, fontsize=10)\n",
        "plt.title(\"Decision Tree Regression Logic\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CWuaOTw5ukkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "WzbbC6xRu_vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "print(f\"Random Forest R^2 Score: {r2_score(y_test, y_pred_rf):.2f}\")"
      ],
      "metadata": {
        "id": "v9DwPy0lu_Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model?\n"
      ],
      "metadata": {
        "id": "LdlbPDvSvH2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Assuming you have run predictions for all models:\n",
        "# y_pred_lin, y_pred_lasso, y_pred_dt, y_pred_rf\n",
        "\n",
        "model_results = [\n",
        "    {\"Model\": \"Linear Regression\", \"MAE\": mean_absolute_error(y_test, y_pred_lin), \"R2\": r2_score(y_test, y_pred_lin)},\n",
        "    {\"Model\": \"Lasso Regression\", \"MAE\": mean_absolute_error(y_test, y_pred_lasso), \"R2\": r2_score(y_test, y_pred_lasso)},\n",
        "    {\"Model\": \"Decision Tree\", \"MAE\": mean_absolute_error(y_test, y_pred_dt), \"R2\": r2_score(y_test, y_pred_dt)},\n",
        "    {\"Model\": \"Random Forest\", \"MAE\": mean_absolute_error(y_test, y_pred_rf), \"R2\": r2_score(y_test, y_pred_rf)}\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "summary_df = pd.DataFrame(model_results)\n",
        "\n",
        "# Sort by R2 (higher is better)\n",
        "summary_df = summary_df.sort_values(by=\"R2\", ascending=False).reset_index(drop=True)\n",
        "print(summary_df)\n",
        "\n",
        "# Automatically identify the winner\n",
        "best_model_name = summary_df.iloc[0]['Model']\n",
        "print(f\"The Best Performing Model is: {best_model_name}\")"
      ],
      "metadata": {
        "id": "VYkVBNITvN7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "So this is generally how you would perform an ML pipeline by testing various models and picking the best-performing one. There is 1 thing we couldn't cover that I'll briefly mention.\n",
        "\n",
        "Other than checking different models against each other, you can also check the same model against itself, but with different parameters. For example, would a random forest regressor with n_estimators=1000 perform better than the one we used with n_estimators=100? There are so many things we could test to optimize our machine learning model; far to many for this workshop.\n",
        "\n",
        "This process of iteratively picking the best parameters is known as \"hyperparameter tuning,\" and is something you would always do."
      ],
      "metadata": {
        "id": "wJrs30ycySND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#That's all folks!"
      ],
      "metadata": {
        "id": "Q-2rSXlczbi3"
      }
    }
  ]
}